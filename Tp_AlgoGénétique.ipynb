{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd8efc83",
   "metadata": {},
   "source": [
    "# importation des bibliothéque\n",
    "\n",
    "On a importer *`MLPClassifier`* from *`sklearn.neural_network`* pour créer un multi layer perceptron pour entraîner des réseaux de neurones artificiels pour la classification,on a aussi importer *`train_test_split`* from *`sklearn.model_selection`* pour deviser la dataset en train et test datasets, aprés on a importer *`dataset`* from *sklearn* car le module *`sklearn.dataset`* contient des dataset.\n",
    "On a importer *`base`* from *`deap`* pour fournir des éléments de base nécessaires pour utiliser la bibliothèque DEAP, *`creator`* pour créer des nouveaux types d'objets évolutifs,*`tools`* pour fournir des outils pour effectuer des operations courantes dans les algorithme évolutifs et *`algorithms`* qui contient des algo évolutifs prédéfinis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6bc06adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from deap import base, creator, tools, algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd31f8a8",
   "metadata": {},
   "source": [
    "# importation de dataset et devision en test et train \n",
    "on a charger la dataset iris en utilisants la fonction *`load_iris()`* de la bibliothéque *datasets*.\n",
    "aprés on a divisé la dataset en un ensemle de *train* et ensemble de *test* en utilisant *`train_test_split()`*.\n",
    "iris.data contient les caractéristiques (longueur et largeur des sépales et des pétales) pour chaque échantillon.\n",
    "iris.target contient les étiquettes de classe (0 pour setosa, 1 pour versicolor et 2 pour virginica) pour chaque échantillon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "692538ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f42816",
   "metadata": {},
   "source": [
    "# Fonction d'évaluation pour l'optimisation d'un modèle de réseau de neurones multicouches (MLP)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f78b893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(individual):\n",
    "    learning_rate = individual[0]\n",
    "    # Création du classifieur MLP avec le taux d'apprentissage spécifié\n",
    "    clf = MLPClassifier(hidden_layer_sizes=(13, 13, 3), max_iter=15000, learning_rate='adaptive',\n",
    "                        learning_rate_init=learning_rate, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    # Prédiction sur l'ensemble de test\n",
    "    y_pred = clf.predict(X_test)\n",
    "    # Calcul de l'accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e362ad00",
   "metadata": {},
   "source": [
    "# Définition de types d'objets évolutifs dans DEAP\n",
    "on a crée un nouveau type d'objet qui s'appel *FItnessMax* en se basent sur *base.Fitness* pour représenter le fitness d'un individu, dans ce cas elle est pondérée avec un poids de 1.0 ce que signifie qu'elle est utiliser pour maximiser une fonction lors de l'optimisation.\n",
    "on a aussi crée un nouveau type d'objet appelé *Individual*. Cet objet est une liste qui représente un individu dans l'algorithme évolutif. La fitness de chaque individu est associée au type \"FitnessMax\" que nous avons déja créé. Cela signifie que chaque individu de type \"Individual\" aura une valeur de fitness qui peut être utilisée pour évaluer sa qualité dans le contexte de l'algorithme évolutif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b37c5d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\deap\\creator.py:185: RuntimeWarning: A class named 'FitnessMax' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\deap\\creator.py:185: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
     ]
    }
   ],
   "source": [
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cc8053",
   "metadata": {},
   "source": [
    "# création de `toolbox` et l'enregistrement des opérateurs\n",
    "On a crée une boîte à outils *toolbox* vide à l'aide de la classe **`base.Toolbox()`**,cette boite pour enregistrer les opérateurs dénétiques et d'autres fonctions nécessaires.\n",
    "Aprés la création de la boite et à l'aide de **`toolbox.register(\"attr_float\", np.random.uniform, 0.0001, 1.0)`**, on a enregistrer une fnction appelée *attr_float* , cette fonction est pour énérer des valeurs al\"atoire de la plage [0.0001, 1.0].\n",
    "on a aussi enregistrer une fonction qui s'appelle *individual* en utilisant **`toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_float, n=1)`** ,Cette fonction crée un individu de type \"Individual\" en appelant la fonction *attr_float* une fois. Cela signifie que chaque individu de la population aura un seul attribut, qui sera un nombre généré aléatoirement de *attr_float*.\n",
    "on a enregistrer une fonction appelée *population* dans la boite *toolbox* à l'aide de **`toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)`** Cette fonction crée une population en répétant l'appel à la fonction \"individual\" pour créer plusieurs individus et les stocker dans une liste.\n",
    "**`toolbox.register(\"evaluate\", evaluate)`** , Cela enregistre la fonction d'évaluation . Cette fonction sera utilisée pour évaluer la fitness de chaque individu de la population.\n",
    "et pour enregistrer l'opérateur de croisement on a utiliser **`toolbox.register(\"mate\", tools.cxBlend, alpha=0.5)`**,cet opérateur utilisé pour croiser les individus lors de la génération.\n",
    "on a enregistrer l'opérateur de mutation dans la boite *toolbox* en utilisant **`toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=0.1, indpb=0.2)`** pour muter les individus de la population en introduisant des variations aléatoires dans leurs attributs.\n",
    "et finalement on a enragistrer l'operateur de selection **`toolbox.register(\"select\", tools.selTournament, tournsize=3)`**, cet operateur est pour séléctionner les individus à reproduire lors de la génération en utilisant un tournoi de taille 3 pour comparer les individus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fffdfdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"attr_float\", np.random.uniform, 0.0001, 1.0)  # Learning rate range: [0.0001, 1.0]\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_float, n=1)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "toolbox.register(\"evaluate\", evaluate)\n",
    "toolbox.register(\"mate\", tools.cxBlend, alpha=0.5)\n",
    "toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=0.1, indpb=0.2)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7518010",
   "metadata": {},
   "source": [
    "# Définition du fonction main\n",
    "\n",
    "on a crée une population de 50 individus,et un objetHallOfFame pour stocker le meilleur individus rencontrée, et quelque statistics functions pour mesurer la performance des individus, puis on a initailiser quelque variables on va l'utiliser par la suite.\n",
    "dans une boucle qui continue jusqu'à la génération=50, à chaque itération de la boucle, de nouveaux individus sont générés en utilisant l'opérateur de variation *`varOr`*. Cela peut inclure la reproduction, le croisement et la mutation des individus de la population actuelle, la fonction d'évaluation est utilisée pour évaluer la fitness de chaque individu de la population et les individus sont ensuite mis à jour avec leurs valeurs de fitness , si la fitness de l'un des individus dépasse la meilleure précision enregistrée jusqu'à présent, la meilleure précision et le taux d'apprentissage correspondant sont mis à jour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b989894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    pop = toolbox.population(n=4)\n",
    "    algorithms.eaSimple(pop, toolbox, cxpb=0.5, mutpb=0.2, ngen=40, verbose=True)\n",
    "    best_individual = tools.selBest(pop, k=1)[0]\n",
    "    best_learning_rate = best_individual[0]\n",
    "    print(\"Meilleur taux d'apprentissage trouvé:\", best_learning_rate)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e440a720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tnevals\n",
      "0  \t4     \n",
      "1  \t4     \n",
      "2  \t3     \n",
      "3  \t2     \n",
      "4  \t2     \n",
      "5  \t1     \n",
      "6  \t2     \n",
      "7  \t4     \n",
      "8  \t3     \n",
      "9  \t4     \n",
      "10 \t2     \n",
      "11 \t3     \n",
      "12 \t2     \n",
      "13 \t1     \n",
      "14 \t4     \n",
      "15 \t2     \n",
      "16 \t0     \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "learning_rate_init must be > 0, got adaptive.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17504\\3832242952.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17504\\3935574408.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mpop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoolbox\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0malgorithms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meaSimple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoolbox\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcxpb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmutpb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mbest_individual\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselBest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mbest_learning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbest_individual\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\deap\\algorithms.py\u001b[0m in \u001b[0;36meaSimple\u001b[1;34m(population, toolbox, cxpb, mutpb, ngen, stats, halloffame, verbose)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[0minvalid_ind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moffspring\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfitness\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[0mfitnesses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoolbox\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoolbox\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minvalid_ind\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minvalid_ind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfitnesses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m             \u001b[0mind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfitness\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17504\\2936077860.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(individual)\u001b[0m\n\u001b[0;32m      4\u001b[0m     clf = MLPClassifier(hidden_layer_sizes=(13, 13, 3), max_iter=15000, learning_rate='adaptive',\n\u001b[0;32m      5\u001b[0m                         learning_rate_init=learning_rate, random_state=42)\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;31m# Prédiction sur l'ensemble de test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    671\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtrained\u001b[0m \u001b[0mMLP\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m         \"\"\"\n\u001b[1;32m--> 673\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, incremental)\u001b[0m\n\u001b[0;32m    355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;31m# Validate input parameters.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_hyperparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_layer_sizes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             raise ValueError(\"hidden_layer_sizes must be > 0, got %s.\" %\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_validate_hyperparameters\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    418\u001b[0m         if (self.learning_rate in [\"constant\", \"invscaling\", \"adaptive\"] and\n\u001b[0;32m    419\u001b[0m                 self.learning_rate_init <= 0.0):\n\u001b[1;32m--> 420\u001b[1;33m             raise ValueError(\"learning_rate_init must be > 0, got %s.\" %\n\u001b[0m\u001b[0;32m    421\u001b[0m                              self.learning_rate)\n\u001b[0;32m    422\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmomentum\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmomentum\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: learning_rate_init must be > 0, got adaptive."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9244fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0641ec2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
